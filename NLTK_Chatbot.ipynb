{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMlycT/N2hDkOmdjO7aYr8H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anjalii02/NLTK_Chatbot/blob/main/NLTK_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "delM2HpOMs2t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('/content/data.txt','r',errors='ignore')\n",
        "raw_doc=f.read()\n"
      ],
      "metadata": {
        "id": "bSTWm29vN9Pi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_doc =raw_doc.lower()\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "sentence_tokens=nltk.sent_tokenize(raw_doc)\n",
        "word_tokens=nltk.word_tokenize(raw_doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dGWR7OwRHG8",
        "outputId": "6ba1d0d7-5318-41ee-ab38-bfc665870002"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5Dd-w7rHSpJh",
        "outputId": "6a2326a6-a28d-4bc5-ff09-e3174d3a93c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'parts of this article (those related to everything, particularly sections after the intro) need to be updated.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-smwD0Sye3ht",
        "outputId": "9f53201b-ec47-4fab-9028-5ea589502e8f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wikipediathe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import remove\n",
        "lemmer=nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "  remove_punct_dict=dict((ord(punct),None) for punct in string.punctuation)\n",
        "  def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "IfIGkeAxe-Ld"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greet_inputs=(\"hello\",\"hi\",\"greeting\",\"sup\",\"what's up\",\"hey\")\n",
        "greet_responses=(\"hi\",\"hey\",\"nods\",\"hi there\",\"hello\",\"I am glad! You are talking to me\")\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet_inputs:\n",
        "      return random.choice(greet_responses)"
      ],
      "metadata": {
        "id": "VdHoYAWSf2jW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "BjveUKY7gGvT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LemNormalize(text):\n",
        "    lemmer = nltk.stem.WordNetLemmatizer()  # Initialize lemmer here\n",
        "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "    return [lemmer.lemmatize(token) for token in nltk.word_tokenize(text.lower().translate(remove_punct_dict))]\n",
        "\n",
        "def response(user_response):\n",
        " robo1_response=''\n",
        " TfidfVec=TfidfVectorizer(tokenizer=LemNormalize,stop_words='english')\n",
        " tfidf=TfidfVec.fit_transform(sentence_tokens)\n",
        " vals=cosine_similarity(tfidf[-1],tfidf)\n",
        " idx=vals.argsort()[0][-2]\n",
        " flat=vals.flatten()\n",
        " flat.sort()\n",
        " req_tfidf=flat[-2]\n",
        " if(req_tfidf==0):\n",
        "   robo1_response=robo1_response+\"I am sorry! I don't understand you\"\n",
        "   return robo1_response\n",
        " else:\n",
        "   robo1_response=robo1_response+sentence_tokens[idx]\n",
        "   return robo1_response"
      ],
      "metadata": {
        "id": "R0K-ySO-gK5Z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag = True\n",
        "print(\"Hello!I am a learning bot. Start typing your text after greeting to talk to me. For ending convo type bye!\")\n",
        "while flag == True:\n",
        "    user_response = input()\n",
        "    user_response = user_response.lower()\n",
        "    if (user_response != 'bye'):\n",
        "        if (user_response == 'thanks' or user_response == 'thank you'):\n",
        "            flag = False\n",
        "            print(\"You are welcome..\")\n",
        "        else:\n",
        "            sentence_tokens.append(user_response)\n",
        "            word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "            final_words = list(set(word_tokens))\n",
        "            print(\"BOT: \", end=\"\")\n",
        "            print(response(user_response))\n",
        "            sentence_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag = False\n",
        "        print(\"Bye! take care..\")\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PAdJF_agvb2",
        "outputId": "0ae4e3e9-dc1c-4ab0-a4e5-434b28fda4a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello!I am a learning bot. Start typing your text after greeting to talk to me. For ending convo type bye!\n",
            "Hi\n",
            "BOT: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi\n",
            "hi\n",
            "BOT: hi\n",
            "how are you\n",
            "BOT: I am sorry! I don't understand you\n",
            "can you tell me about virtual assistant chatbot\n",
            "BOT: [8]\n",
            "\n",
            "a major area where chatbots have long been used is in customer service and support, with various sorts of virtual assistants.\n",
            "main menu\n",
            "BOT: \n",
            "main menu\n",
            "\n",
            "wikipediathe free encyclopedia\n",
            "search\n",
            "\n",
            "appearance\n",
            "create account\n",
            "log in\n",
            "\n",
            "personal tools\n",
            "\n",
            "toggle the table of contents\n",
            "chatbot\n",
            "\n",
            "article\n",
            "talk\n",
            "read\n",
            "edit\n",
            "view history\n",
            "\n",
            "tools\n",
            "from wikipedia, the free encyclopedia\n",
            "for the bot-creation software, see chatbot.\n",
            "bye\n",
            "Bye! take care..\n"
          ]
        }
      ]
    }
  ]
}